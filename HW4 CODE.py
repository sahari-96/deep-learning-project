# -*- coding: utf-8 -*-
"""Untitled112.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10sfxOy7SuKMwmVHJ2g4F6YYiCU7cLCsr
"""

#در این قسمت کتابخانه های هضم و فست تکست و مدل استفاده شده در فست تکست برای امبدینگ کلمات فارسی را دانلود میکنیم
!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fa.300.bin.gz
!gunzip cc.fa.300.bin.gz

!pip install fasttext


import fasttext 

model = fasttext.load_model("cc.fa.300.bin")

!pip install hazm

#در این قسمت ابتدا داده ها را از اکسل که در فضای درایو هست میگیریم و دو صفحه ی ان را یکی میکنیم
import fasttext 
import pandas
import random
import numpy
import hazm

from google.colab import drive
drive.mount('/content/drive')

path=('/content/drive/MyDrive/Labeled-Sentences.xlsx')

data0=pandas.read_excel(path,sheet_name=0)
data1=pandas.read_excel(path,sheet_name=1)

data=pandas.DataFrame({'review':list(data0['review'])+list(data1['Unnamed: 0']),'label':list(data0['label'])+list(data1['label'])})
 
 #اینجا مقادیر نال را حذف میکنیم
data.dropna(subset = ["review"], inplace=True)
print(data)

# این قسمت با استفاده از کتابخانه هضم قسمت ریویو ها یعنی متن نظرات را نرمال سازی میکنیم. مانند درست کردن نیم فاصله ها و حذف کردن اموجی ها و حروف رابطه و غیره    
def CleanPersianText(text):
  _normalizer = hazm.Normalizer()
  text = _normalizer.normalize(text)
  return text

#در این قسمت یک لیست از متن های نرمال شده و لیبل ها درست میکنیم
revlist = list(map(lambda x: [CleanPersianText(x[0]), (x[1])],zip(data['review'],data['label'])))

print(len(revlist))

print(revlist)

#  در این قسمت نوع لیبل ها را مشخص میکینم و تعداد پراکندگی هر کدام از انها را چاپ میکنیم
excellent=list(filter(lambda x: x[1] == 2,revlist))
good=list(filter(lambda x: x[1] == 1,revlist))
bad=list(filter(lambda x: x[1] == 0,revlist))


print("excellent count {}".format(len(excellent)))
print("good count {}".format(len(good)))
print("bad  count {}".format(len(bad)))

#در این قسمت تعداد حداکثر توکن ها و سایز بردار های عددی که از کلمات ساخته میشود را مشخص میکنیم
import numpy as np
import keras.backend as K

vector_size = 300 
max_no_tokens = 500

#در این قسمت تعداد داده های تست و ترین را مشخص میکنیم
train_size = int(0.8*(len(revlist)))
test_size = int(0.2*(len(revlist)))



indexes = set(np.random.choice(len(revlist), train_size + test_size, replace=False))

x_train = np.zeros((train_size, max_no_tokens, vector_size), dtype=K.floatx())
y_train = np.zeros((train_size, 3), dtype=np.int32)

x_test = np.zeros((test_size, max_no_tokens, vector_size), dtype=K.floatx())
y_test = np.zeros((test_size, 3), dtype=np.int32)


# در این قسمت با استفاده از مدلی که در قسمت اول کد از فست تکست دانلود کردیم توکن ها را به بردار های عددی تبدیل میکنیم
for i, index in enumerate(indexes):
  text_words = hazm.word_tokenize(revlist[index][0])
  for t in range(0,len(text_words)):
    if t >= max_no_tokens:
      break
    
    if text_words[t] not in model.words:
      continue
    if i < train_size:
      x_train[i, t, :] = model.get_word_vector(text_words[t])
    else:
      x_test[i - train_size, t, :] = model.get_word_vector(text_words[t])


#در این قسمت مدل نمایش لیبل ها را مشخص میکنیم
  if i < train_size:
    if revlist[index][1] == 0:
      y_train[i, :] = [1.0, 0.0, 0.0] 
    if revlist[index][1] == 1:
      y_train[i, :] = [0.0, 1.0, 0.0]
    if revlist[index][1] == 2:
      y_train[i, :] = [0.0, 0.0, 1.0]

  else:
    if revlist[index][1] == 0:
      y_test[i - train_size, :] = [1.0, 0.0, 0.0]
    if revlist[index][1] == 1:
      y_test[i - train_size, :] = [0.0, 1.0, 0.0]
    if revlist[index][1] == 2:
      y_test[i - train_size, :] = [0.0, 0.0, 1.0]

#در این قسمت شبکه ی خود را با کراس درست میکنیم و مدل خود را روی داده تست ارزیابی میکنیم و مقدار دقت و خطا و نمودار ان ها را رسم میکنیم
batch_size = 250 
no_epochs = 10

from keras.models import Sequential
from keras.layers import Conv1D, Dropout, Dense, Flatten, LSTM, MaxPooling1D, Bidirectional, Embedding
from keras.optimizers import Adam
from keras.callbacks import EarlyStopping, TensorBoard

#این قسمت مربوط به شبکه با یک لایه ال اس تی ام است
DeepModel = Sequential()

DeepModel.add(LSTM(512, input_shape=(max_no_tokens, vector_size)))#, return_sequences= True))
DeepModel.add(Dropout(0.25))
DeepModel.add(Dense(512, activation='sigmoid'))
DeepModel.add(Dropout(0.25))
DeepModel.add(Dense(3, activation='softmax'))



'''این قسمت مربوط به شبکه بازگشتی ال اس تی ام دو طرفه است
DeepModel.add(Conv1D(32, kernel_size=3, activation='elu', padding='same',
                 input_shape=(max_no_tokens, vector_size)))
DeepModel.add(Bidirectional(LSTM(512, dropout=0.2, recurrent_dropout=0.3)))
DeepModel.add(Dense(512, activation='sigmoid'))
DeepModel.add(Dropout(0.25))
DeepModel.add(Dense(3, activation='softmax'))
'''


'''
این قسمت مربوط به قسمت امتیازی شبکه کانولوشن است
DeepModel.add(Conv1D(32, kernel_size=3, activation='elu', padding='same',
                 input_shape=( max_no_tokens, vector_size)))
DeepModel.add(Conv1D(32, kernel_size=3, activation='elu', padding='same'))
DeepModel.add(Conv1D(32, kernel_size=3, activation='relu', padding='same'))
DeepModel.add(MaxPooling1D(pool_size=3))
DeepModel.add(Dropout(0.25))
DeepModel.add(Flatten())
DeepModel.add(Dense(512, activation='sigmoid'))
DeepModel.add(Dropout(0.2))
DeepModel.add(Dense(512, activation='sigmoid'))
DeepModel.add(Dropout(0.25))
DeepModel.add(Dense(512, activation='sigmoid'))
DeepModel.add(Dropout(0.25))
DeepModel.add(Dense(3, activation='softmax'))
'''



DeepModel.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.0001, decay=1e-6), metrics=['accuracy'])


DeepModel.summary()

history = DeepModel.fit(x_train, y_train, batch_size=batch_size, shuffle=True, epochs=no_epochs,validation_data=(x_test, y_test))

import matplotlib.pyplot as plt

plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label = 'val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0.5, 1])
plt.legend(loc='lower right')
plt.show()

plt.plot(history.history['loss'], label='loss')
plt.plot(history.history['val_loss'], label = 'val_loss')
plt.xlabel('Epoch')
plt.ylabel('loss')
plt.ylim([0.5, 1])
plt.legend(loc='lower right')
plt.show()

test_loss, test_acc = DeepModel.evaluate(x=x_test, y=y_test, batch_size=32, verbose=1)
print('test_loss =', test_loss)
print("test _acc =", test_acc)



DeepModel.save('mymodel.model')

#در این قسمت متنی را گرفته و مدل ما لیبل ان را سعی میکند پیشبینی کند
user_text = "امکانات هتل عالی بود"
from IPython.core.display import display, HTML
_normalizer = hazm.Normalizer()
if not user_text=="":
  text_for_test = _normalizer.normalize(user_text)
  text_for_test_words = hazm.word_tokenize(text_for_test)
  x_text_for_test_words = np.zeros((1,max_no_tokens,vector_size),dtype=K.floatx())
  for t in range(0,len(text_for_test_words)):
    if t >= max_no_tokens:
      break
    if text_for_test_words[t] not in model.words:
      continue
    
    x_text_for_test_words[0, t, :] = model.get_word_vector(text_for_test_words[t])
  
  result = DeepModel.predict(x_text_for_test_words)
  print(numpy.argmax(result))